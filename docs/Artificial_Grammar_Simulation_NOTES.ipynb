{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "GENERAL INFORMATION    -  ARCHITECTURE\n",
        "\n",
        "\n",
        "\n",
        "The metacognitive network consists of 2 interconnected networks:\n",
        "\n",
        "1. A three layers backpropagation feddforward first-order network ( MAIN TASK)\n",
        "2. A second order network that continously evaluates the performance of the first order network   =   hidden unit  +   two output nodes(HIGH or LOW wagers)\n",
        "\n",
        "*In both cases a winner-take-all algorithm in the output was used (all output activations ranging from 0.0. to 1.0)\n",
        "\n",
        "\n",
        "Main Differences of the particular metacognitive network:\n",
        "\n",
        "1. implemented\n",
        "the distinction between low and high awareness conditions\n",
        "by manipulating the first-order network’s training\n",
        "phase length (short and long training phases corresponding\n",
        "to implicit and explicit conditions, respectively"
      ],
      "metadata": {
        "id": "rZG8quwc84Yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ARCHITECTURE - FIRST ORDER NETWORK\n",
        "\n",
        "1. The first order network is an autoassociator, complemented by a winner-take-all at the output = Re-representation of the input = Creates a bimodal distribution of inputs\n",
        "\n",
        "*What a non-conscious learning mechanism can do\n",
        "\n",
        "\n",
        "- It is a backpropagation autoassociator\n",
        "- First, it consists of a 48-unit input layer ( representing a string of minimum 3 items and maximum 8, each being one of 5 possible letters, constructed according to the selected artificial grammar)\n",
        "- It is connected to a layer of 40 hidden units\n",
        "- Then, it is finally connected to a 48-unit output layer\n",
        "-Initial connection weights were between -1.0 and 1.0\n"
      ],
      "metadata": {
        "id": "vhWWvCXH-Btv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ARCHITECTURE - SECOND ORDER NETWORK\n",
        "\n",
        "2. The second order network's hidden units consists of a comparator matrix\n",
        "\n",
        "- The input is a 48-unit comparison matrix ( match between the first-order input and output layers )\n",
        "\n",
        "- Initial connection weights were between 0.0 and 0.1"
      ],
      "metadata": {
        "id": "ERi77z9CE0YF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROPERTIES\n",
        "1. development of the comparison patterns is automatic and unsupervised (that is, not driven by feedback)\n",
        "2. the second-order network’s access to these patterns for wagering is learned in a pre-training phase, independent of specific first-order patterns or training and testing tasks."
      ],
      "metadata": {
        "id": "48QXKzH-Cmpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------\n",
        "PATTERNS\n",
        "\n",
        "- 80 Random patters were used at first fro pre-training\n",
        "\n",
        "- For training, 75 patterns were used according to grammar A.\n",
        "\n",
        "- For training, 30 patterns were used according to grammar B.\n",
        "--------------------------------------"
      ],
      "metadata": {
        "id": "-jmf9tyTE6el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------\n",
        "PRE-TRAINING PHASE\n",
        "\n",
        "1. The netwrok was subjected to training on the 80 random patterns for 60 epochs\n",
        "2. half of the patterns were accompanied by learning in the first-order network, while the other half were not\n",
        "3. the second-order network had to wager high when first-order input and output matched, and low when they did not\n",
        "4. Following pre-training, all first-order network’s connections were reset to initial conditions, whereas second-order network’s weights were kept as was until the end of the simulation.\n",
        "------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "6WW_9gb3C0G3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------\n",
        "TRAININING PHASE\n",
        "\n",
        "1. only the first-order networks were trained again on 45 patterns of Grammar A.\n",
        "2. Of 30 networks, 15 were assigned to a “High Consciousness”(SUPRATHRESHOLD) condition and were trained for 12 epochs\n",
        "3. 15 networks in the “Low Consciousness” (SUBTHRESHOLD)condition were trained for only 3 epochs\n",
        "4. first-order network’s and second-order network’s learning rates were set to 0.4, units’ momentum was 0.5, and temperature was 1.0.\n",
        "\n",
        "------------------------------------------------------------------------------\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uq2ZAh_QYQio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------\n",
        "TESTING PHASE\n",
        "\n",
        "1. tested all 30 networks on the same set of 60 patterns, consisting of 30 novel Grammar A patterns, and 30 Grammar B patterns\n",
        "----------------------------------------------"
      ],
      "metadata": {
        "id": "CewDZA1ZH6V1"
      }
    }
  ]
}